from __future__ import annotations

import json
from typing import Any, Dict, List, Optional

from fastapi import HTTPException
from sqlalchemy.orm import Session

from app.core.config import settings
from app.schemas.tutor import TutorChatData, TutorGenerateQuestionsData
from app.services.user_service import ensure_user_exists
from app.services.corrective_rag import corrective_retrieve_and_log
from app.services.rag_service import auto_document_ids_for_query
from app.services.text_quality import filter_chunks_by_quality
from app.services.llm_service import llm_available, chat_json, pack_chunks
from app.services.quiz_service import clean_mcq_questions, _generate_mcq_from_chunks
from app.services.topic_service import build_topic_details


def _src_preview(text: str, n: int = 180) -> str:
    s = " ".join(str(text or "").split())
    if len(s) <= n:
        return s
    return s[: n - 1].rstrip() + "‚Ä¶"


def tutor_chat(
    db: Session,
    *,
    user_id: int,
    question: str,
    topic: Optional[str] = None,
    top_k: int = 6,
    document_ids: Optional[List[int]] = None,
) -> Dict[str, Any]:
    """Virtual AI Tutor (RAG). Answers using only retrieved evidence and suggests follow-ups."""

    ensure_user_exists(db, int(user_id), role="student")

    q = (question or "").strip()
    if not q:
        raise HTTPException(status_code=422, detail="Missing question")

    # Auto-scope to teacher docs by default
    doc_ids = list(document_ids or [])
    if not doc_ids:
        auto = auto_document_ids_for_query(db, topic or q, preferred_user_id=settings.DEFAULT_TEACHER_ID, max_docs=3)
        if auto:
            doc_ids = auto

    filters = {"document_ids": doc_ids} if doc_ids else {}
    query = f"{topic.strip()}: {q}" if topic and topic.strip() else q

    rag = corrective_retrieve_and_log(
        db=db,
        query=query,
        top_k=int(max(3, min(20, top_k))),
        filters=filters,
        topic=topic,
    )

    # If retrieval is clearly irrelevant, politely refuse (user requirement: tutor should not answer off-topic).
    corr = rag.get("corrective") or {}
    attempts = corr.get("attempts") or []
    last_try = attempts[-1] if isinstance(attempts, list) and attempts else {}
    try:
        best_rel = float(last_try.get("best_relevance", 0.0) or 0.0)
    except Exception:
        best_rel = 0.0

    chunks = rag.get("chunks") or []
    # Heuristic: if even the best chunk barely matches the query, we treat it as out-of-scope.
    # (CRAG grading is lexical so we keep the threshold permissive.)
    try:
        import re

        q_words = len(re.findall(r"[\w√Ä-·ªπ]+", query or ""))
    except Exception:
        q_words = 0
    if chunks and q_words >= 2 and best_rel < float(settings.CRAG_MIN_RELEVANCE) * 0.55:
        answer_md = (
            "M√¨nh ch·ªâ c√≥ th·ªÉ tr·∫£ l·ªùi d·ª±a tr√™n *t√†i li·ªáu gi√°o vi√™n ƒë√£ upload*.\n\n"
            "C√¢u h·ªèi n√†y c√≥ v·∫ª **ngo√†i ph·∫°m vi t√†i li·ªáu hi·ªán t·∫°i** (m√¨nh kh√¥ng t√¨m th·∫•y ƒëo·∫°n n√†o li√™n quan ƒë·ªß ch·∫Øc ch·∫Øn).\n\n"
            "üëâ G·ª£i √Ω: h√£y ch·ªçn ƒë√∫ng *t√†i li·ªáu* ·ªü dropdown, ho·∫∑c n√™u r√µ *topic/b√†i* b·∫°n ƒëang h·ªçc, r·ªìi h·ªèi l·∫°i."
        )
        return TutorChatData(
            answer_md=answer_md,
            follow_up_questions=[
                "B·∫°n ƒëang h·ªçc b√†i/ ch∆∞∆°ng n√†o trong t√†i li·ªáu?",
                "B·∫°n c√≥ th·ªÉ tr√≠ch 1-2 c√¢u trong t√†i li·ªáu li√™n quan ƒë·ªÉ m√¨nh gi·∫£i th√≠ch kh√¥ng?",
                "B·∫°n mu·ªën m√¨nh gi·∫£i th√≠ch kh√°i ni·ªám hay l√†m b√†i t·∫≠p theo v√≠ d·ª• trong t√†i li·ªáu?",
            ],
            quick_check_mcq=[],
            sources=[],
            retrieval=rag.get("corrective") or {},
        ).model_dump()
    good, bad = filter_chunks_by_quality(chunks, min_score=float(settings.OCR_MIN_QUALITY_SCORE))
    bad_ratio = float(len(bad)) / float(max(1, len(chunks)))
    if (not good) or (bad_ratio >= float(settings.OCR_BAD_CHUNK_RATIO) and len(good) < 2):
        # ChatGPT-like graceful fallback: do NOT hard-error the UI.
        msg = (
            "M√¨nh ch∆∞a th·ªÉ tr·∫£ l·ªùi ch·∫Øc ch·∫Øn v√¨ ph·∫ßn t√†i li·ªáu m√¨nh truy xu·∫•t ƒë∆∞·ª£c ƒëang b·ªã **l·ªói OCR / r·ªùi r·∫°c** (ch·ªØ b·ªã v·ª°, thi·∫øu d·∫•u, sai d√≤ng).\n\n"
            "B·∫°n c√≥ th·ªÉ l√†m 1 trong c√°c c√°ch sau ƒë·ªÉ m√¨nh tr·∫£ l·ªùi chi ti·∫øt h∆°n:\n"
            "1) Upload l·∫°i file **.docx** ho·∫∑c PDF c√≥ **text layer** (copy ƒë∆∞·ª£c ch·ªØ).\n"
            "2) Copy-paste ƒë√∫ng ƒëo·∫°n li√™n quan (kho·∫£ng 10‚Äì30 d√≤ng) v√†o √¥ chat.\n"
            "3) N√™u r√µ *ch∆∞∆°ng/m·ª•c* + *t·ª´ kho√°* ƒë·ªÉ m√¨nh l·ªçc ƒë√∫ng ph·∫ßn.\n\n"
            "N·∫øu b·∫°n g·ª≠i l·∫°i c√¢u h·ªèi k√®m 1 ƒëo·∫°n tr√≠ch, m√¨nh s·∫Ω gi·∫£i th√≠ch t·ª´ng b∆∞·ªõc nh∆∞ gi√°o vi√™n." 
        )

        return TutorChatData(
            answer_md=msg,
            follow_up_questions=[
                "B·∫°n ƒëang h·ªèi trong ch∆∞∆°ng/m·ª•c n√†o c·ªßa t√†i li·ªáu?",
                "B·∫°n c√≥ th·ªÉ d√°n ƒëo·∫°n vƒÉn li√™n quan (10‚Äì30 d√≤ng) kh√¥ng?",
                "B·∫°n mu·ªën m√¨nh gi·∫£i th√≠ch theo ki·ªÉu: ƒë·ªãnh nghƒ©a ‚Üí v√≠ d·ª• ‚Üí l·ªói th∆∞·ªùng g·∫∑p hay theo b√†i t·∫≠p?",
            ],
            quick_check_mcq=[],
            sources=[],
            retrieval={
                **(rag.get("corrective") or {}),
                "note": "OCR_QUALITY_TOO_LOW",
                "bad_chunk_ratio": bad_ratio,
                "good": len(good),
                "total": len(chunks),
                "sample_bad": bad[:2],
            },
        ).model_dump()
    chunks = good

    # Build sources for UI/debug
    sources = []
    for c in chunks[: min(len(chunks), int(top_k))]:
        sources.append(
            {
                "chunk_id": int(c.get("chunk_id")),
                "document_id": int(c.get("document_id")) if c.get("document_id") is not None else None,
                "document_title": c.get("document_title") or c.get("title"),
                "score": float(c.get("score", 0.0) or 0.0),
                "preview": _src_preview(c.get("text") or ""),
                "meta": c.get("meta") or {},
            }
        )

    # Default: generate a tiny quick-check MCQ (offline) from the same chunks
    quick_mcq = []
    try:
        quick_mcq = _generate_mcq_from_chunks(topic=topic or "t√†i li·ªáu", level="beginner", question_count=2, chunks=chunks)
        quick_mcq = clean_mcq_questions(quick_mcq, limit=2)
    except Exception:
        quick_mcq = []

    if llm_available():
        packed = pack_chunks(chunks, max_chunks=min(4, len(chunks)), max_chars_per_chunk=750, max_total_chars=2800)
        sys = (
            "B·∫°n l√† Virtual AI Tutor (tr·ª£ gi·∫£ng) cho h·ªçc sinh, phong c√°ch gi·ªëng ChatGPT nh∆∞ng ph·∫£i b√°m t√†i li·ªáu.\n"
            "CH·ªà d·ª±a tr√™n evidence_chunks (kh√¥ng d√πng ki·∫øn th·ª©c ngo√†i). Kh√¥ng b·ªãa. Kh√¥ng copy nguy√™n vƒÉn d√†i.\n\n"
            "Y√™u c·∫ßu tr·∫£ l·ªùi (answer_md) ph·∫£i CHI TI·∫æT, c√≥ c·∫•u tr√∫c nh∆∞ gi√°o vi√™n gi·∫£ng b√†i:\n"
            "1) Tr·∫£ l·ªùi ng·∫Øn g·ªçn (1‚Äì2 c√¢u)\n"
            "2) Gi·∫£i th√≠ch chi ti·∫øt theo t·ª´ng √Ω/b∆∞·ªõc\n"
            "3) V√≠ d·ª• minh ho·∫°: n·∫øu evidence kh√¥ng c√≥ v√≠ d·ª• c·ª• th·ªÉ, h√£y ghi r√µ l√† v√≠ d·ª• gi·∫£ ƒë·ªãnh\n"
            "4) L·ªói th∆∞·ªùng g·∫∑p / l∆∞u √Ω\n"
            "5) T√≥m t·∫Øt 3 √Ω\n\n"
            "N·∫øu evidence kh√¥ng ƒë·ªß ƒë·ªÉ tr·∫£ l·ªùi ch·∫Øc ch·∫Øn: h√£y tr·∫£ l·ªùi KH√âO (l·ªãch s·ª±), n√≥i r√µ thi·∫øu ch·ªó n√†o trong t√†i li·ªáu, "
            "ƒë·∫∑t 1‚Äì3 c√¢u h·ªèi ƒë·ªÉ l√†m r√µ v√† g·ª£i √Ω h·ªçc sinh t√¨m ƒë√∫ng ch∆∞∆°ng/m·ª•c.\n\n"
            "L∆∞u √Ω: evidence_chunks ƒë√£ ƒë∆∞·ª£c rerank theo m·ª©c ƒë·ªô li√™n quan v·ªõi c√¢u h·ªèi. "
            "∆Øu ti√™n d√πng c√°c chunk ·ªü ƒë·∫ßu danh s√°ch; n·∫øu c√°c chunk m√¢u thu·∫´n/kh√°c nhau, ph·∫£i n√™u r√µ v√† ch·ªçn c√¢u tr·∫£ l·ªùi an to√†n nh·∫•t."
        )
        user = {
            "question": q,
            "topic": (topic or "").strip() or None,
            "evidence_chunks": packed,
            "output_format": {
                "answer_md": "markdown",
                "follow_up_questions": ["string"],
                "quick_check_mcq": [
                    {
                        "type": "mcq",
                        "stem": "string",
                        "options": ["A", "B", "C", "D"],
                        "correct_index": 0,
                        "explanation": "string"
                    }
                ],
            },
        }
        try:
            resp = chat_json(
                messages=[
                    {"role": "system", "content": sys},
                    {"role": "user", "content": json.dumps(user, ensure_ascii=False)},
                ],
                temperature=0.25,
                max_tokens=1200,
            )
            if isinstance(resp, dict) and (resp.get("answer_md") or "").strip():
                answer_md = (resp.get("answer_md") or "").strip()
                fu = [str(x).strip() for x in (resp.get("follow_up_questions") or []) if str(x).strip()]
                mcq = resp.get("quick_check_mcq") or []
                if isinstance(mcq, list) and mcq:
                    try:
                        mcq = clean_mcq_questions(mcq, limit=2)
                    except Exception:
                        mcq = []
                else:
                    mcq = quick_mcq

                data = TutorChatData(
                    answer_md=answer_md,
                    follow_up_questions=fu[:3],
                    quick_check_mcq=mcq[:2],
                    sources=sources,
                    retrieval=rag.get("corrective") or {},
                ).model_dump()
                return data
        except Exception:
            pass

    # Offline fallback: stitch a short answer from top chunks (extractive summary)
    bullets = []
    for c in chunks[:3]:
        txt = " ".join(str(c.get("text") or "").split())
        if len(txt) > 260:
            txt = txt[:257].rstrip() + "‚Ä¶"
        if txt:
            bullets.append(f"- {txt}")
    answer_md = (
        (
            "M√¨nh ƒëang ·ªü ch·∫ø ƒë·ªô **kh√¥ng d√πng LLM**, n√™n m√¨nh s·∫Ω tr√≠ch c√°c ƒëo·∫°n li√™n quan nh·∫•t trong t√†i li·ªáu ƒë·ªÉ b·∫°n t·ª± ƒë·ªëi chi·∫øu:\n\n"
            + "\n".join(bullets)
            + "\n\nN·∫øu b·∫°n mu·ªën m√¨nh gi·∫£i th√≠ch chi ti·∫øt h∆°n: h√£y b·∫≠t LLM ho·∫∑c d√°n ƒëo·∫°n vƒÉn c·ª• th·ªÉ (10‚Äì30 d√≤ng)."
        )
        if bullets
        else (
            "M√¨nh **ch∆∞a ƒë·ªß th√¥ng tin trong t√†i li·ªáu** ƒë·ªÉ tr·∫£ l·ªùi ch·∫Øc ch·∫Øn c√¢u n√†y.\n\n"
            "B·∫°n h√£y cho m√¨nh th√™m: (1) ch∆∞∆°ng/m·ª•c ƒëang h·ªçc, ho·∫∑c (2) 1 ƒëo·∫°n tr√≠ch li√™n quan ‚Äî m√¨nh s·∫Ω gi·∫£i th√≠ch ti·∫øp."
        )
    )

    data = TutorChatData(
        answer_md=answer_md,
        follow_up_questions=[],
        quick_check_mcq=quick_mcq,
        sources=sources,
        retrieval=rag.get("corrective") or {},
    ).model_dump()
    return data


def tutor_generate_questions(
    db: Session,
    *,
    user_id: int,
    topic: str,
    level: str | None = None,
    question_count: int = 6,
    top_k: int = 8,
    document_ids: Optional[List[int]] = None,
) -> Dict[str, Any]:
    """Generate a *fresh* set of practice questions from the teacher's documents.

    Design goal (per user requirement): questions are NOT based on a fixed framework.
    The system should discover what is in the document for the chosen topic and ask
    suitable questions (definitions / steps / formulas / examples / pitfalls / comparisons...).
    """

    ensure_user_exists(db, int(user_id), role="student")

    t = (topic or "").strip()
    if not t:
        raise HTTPException(status_code=422, detail="Missing topic")

    qc = int(question_count or 0)
    qc = max(1, min(20, qc))

    # Auto-scope to teacher docs by default
    doc_ids = list(document_ids or [])
    if not doc_ids:
        auto = auto_document_ids_for_query(db, t, preferred_user_id=settings.DEFAULT_TEACHER_ID, max_docs=3)
        if auto:
            doc_ids = auto

    filters = {"document_ids": doc_ids} if doc_ids else {}

    # Retrieval query: keep it simple (topic only) to avoid imposing a template.
    rag = corrective_retrieve_and_log(
        db=db,
        query=t,
        top_k=int(max(6, min(30, top_k))),
        filters=filters,
        topic=t,
    )

    chunks = rag.get("chunks") or []
    good, bad = filter_chunks_by_quality(chunks, min_score=float(settings.OCR_MIN_QUALITY_SCORE))
    bad_ratio = float(len(bad)) / float(max(1, len(chunks)))
    if (not good) or (bad_ratio >= float(settings.OCR_BAD_CHUNK_RATIO) and len(good) < 2):
        raise HTTPException(
            status_code=422,
            detail={
                "code": "NEED_CLEAN_TEXT",
                "message": "CONTEXT b·ªã l·ªói OCR / r·ªùi r·∫°c n√™n kh√¥ng th·ªÉ sinh c√¢u h·ªèi ch·∫Øc ch·∫Øn.",
                "reason": f"bad_chunk_ratio={bad_ratio:.2f}, good={len(good)}, total={len(chunks)}",
                "suggestion": "H√£y upload file .docx ho·∫∑c PDF c√≥ text layer / ho·∫∑c copy-paste ƒë√∫ng m·ª•c c·∫ßn luy·ªán.",
                "debug": {"sample_bad": bad[:2]},
            },
        )
    chunks = good

    # Build sources for UI/debug
    sources = []
    for c in chunks[: min(len(chunks), int(top_k))]:
        sources.append(
            {
                "chunk_id": int(c.get("chunk_id")),
                "document_id": int(c.get("document_id")) if c.get("document_id") is not None else None,
                "document_title": c.get("document_title") or c.get("title"),
                "score": float(c.get("score", 0.0) or 0.0),
                "preview": _src_preview(c.get("text") or ""),
                "meta": c.get("meta") or {},
            }
        )

    packed = pack_chunks(chunks, max_chunks=min(8, len(chunks)), max_chars_per_chunk=900, max_total_chars=5200)
    valid_ids = [int(c["chunk_id"]) for c in packed] if packed else []

    # Build a compact "topic profile" so the LLM can ask questions based on what's actually in the text.
    body_for_profile = "\n\n".join([str(c.get("text") or "") for c in packed]) if packed else ""
    topic_profile = build_topic_details(body_for_profile, title=t) if body_for_profile.strip() else {
        "title": t,
        "outline": [],
        "key_points": [],
        "definitions": [],
        "examples": [],
        "formulas": [],
        "faq": [],
        "misconceptions": [],
        "exercises": [],
    }

    def _tok(s: str) -> set[str]:
        s = (s or "").lower()
        return {w for w in __import__("re").findall(r"[\w√Ä-·ªπ]+", s) if len(w) >= 3}

    def _best_sources(text_hint: str, k: int = 2) -> List[Dict[str, int]]:
        if not packed:
            return []
        hint = _tok(text_hint)
        scored = []
        for c in packed:
            cid = int(c.get("chunk_id"))
            ct = _tok(f"{c.get('title') or ''} {c.get('text') or ''}")
            scored.append((len(hint & ct), cid))
        scored.sort(reverse=True)
        picked = [cid for score, cid in scored if score > 0][:k]
        if not picked:
            picked = [int(packed[0]["chunk_id"])]
        return [{"chunk_id": int(x)} for x in picked]

    # LLM path: generate varied questions WITHOUT a fixed framework.
    if llm_available() and packed:
        sys = (
            "B·∫°n l√† tr·ª£ gi·∫£ng. Nhi·ªám v·ª•: sinh b·ªô C√ÇU H·ªéI LUY·ªÜN T·∫¨P d·ª±a CH·ªà tr√™n evidence_chunks. "
            "Quan tr·ªçng: KH√îNG d√πng m·ªôt 'khung s·∫µn' (v√≠ d·ª•: lu√¥n h·ªèi ƒë·ªãnh nghƒ©a ‚Üí quy tr√¨nh ‚Üí ∆∞u/nh∆∞·ª£c...). "
            "H√£y ƒë·ªçc topic_profile v√† t·ª± ch·ªçn g√≥c h·ªèi ph√π h·ª£p v·ªõi n·ªôi dung th·∫≠t s·ª± c√≥ trong vƒÉn b·∫£n. "
            "N·∫øu topic_profile cho th·∫•y c√≥ quy tr√¨nh/b∆∞·ªõc l√†m, h√£y h·ªèi v·ªÅ b∆∞·ªõc/ƒëi·ªÅu ki·ªán; n·∫øu c√≥ c√¥ng th·ª©c, h·ªèi √Ω nghƒ©a v√† c√°ch √°p d·ª•ng; "
            "n·∫øu c√≥ v√≠ d·ª•/t√¨nh hu·ªëng, h·ªèi ph√¢n t√≠ch; n·∫øu c√≥ l·ªói th∆∞·ªùng g·∫∑p/misconceptions, h·ªèi c√°ch ph√°t hi·ªán/s·ª≠a. "
            "Kh√¥ng b·ªãa ki·∫øn th·ª©c ngo√†i CONTEXT. Kh√¥ng copy nguy√™n vƒÉn d√†i."
        )

        user = {
            "topic": t,
            "level": (level or "").strip() or None,
            "question_count": qc,
            "topic_profile": topic_profile,
            "evidence_chunks": packed,
            "output_format": {
                "status": "OK|NEED_CLEAN_TEXT",
                "questions": [
                    {
                        "type": "open_ended",
                        "stem": "string",
                        "hints": ["string"],
                        "sources": [{"chunk_id": 123}],
                    }
                ],
            },
            "constraints": [
                "M·ªói c√¢u h·ªèi ph·∫£i b√°m √≠t nh·∫•t 1 chunk_id trong evidence_chunks (sources).",
                "C√¢u h·ªèi ph·∫£i c·ª• th·ªÉ, c√≥ y√™u c·∫ßu r√µ r√†ng, tr√°nh m∆° h·ªì.",
                "Kh√¥ng nh·∫Øc c√°c t·ª´: chunk, evidence, tr√≠ch, theo t√†i li·ªáu.",
                "C√°c c√¢u ph·∫£i ƒëa d·∫°ng v√† PH√ô H·ª¢P v·ªõi n·ªôi dung, kh√¥ng l·∫∑p √Ω.",
            ],
        }

        try:
            resp = chat_json(
                messages=[
                    {"role": "system", "content": sys},
                    {"role": "user", "content": json.dumps(user, ensure_ascii=False)},
                ],
                temperature=0.35,
                max_tokens=1600,
            )
        except Exception:
            resp = None

        if isinstance(resp, dict) and str(resp.get("status", "")).upper() == "NEED_CLEAN_TEXT":
            raise HTTPException(
                status_code=422,
                detail={
                    "code": "NEED_CLEAN_TEXT",
                    "message": "CONTEXT kh√¥ng ƒë·ªß r√µ ƒë·ªÉ sinh c√¢u h·ªèi b√°m t√†i li·ªáu.",
                    "reason": resp.get("reason") or resp.get("message") or "CONTEXT b·ªã r·ªùi r·∫°c/k√Ω t·ª± l·ªói ho·∫∑c thi·∫øu th√¥ng tin ch·∫Øc ch·∫Øn.",
                    "suggestion": resp.get("suggestion") or "H√£y upload file .docx ho·∫∑c PDF c√≥ text layer / ho·∫∑c copy text c·ªßa m·ª•c c·∫ßn luy·ªán.",
                },
            )

        raw_qs = resp.get("questions") if isinstance(resp, dict) else None
        if isinstance(raw_qs, list) and raw_qs:
            cleaned = []
            seen = set()
            for q in raw_qs:
                if not isinstance(q, dict):
                    continue
                stem = " ".join(str(q.get("stem") or "").split()).strip()
                if len(stem) < 12:
                    continue
                key = stem.lower()
                if key in seen:
                    continue
                seen.add(key)

                hints = [" ".join(str(x).split()).strip() for x in (q.get("hints") or []) if str(x).strip()]
                sources_raw = q.get("sources")
                if isinstance(sources_raw, dict):
                    sources_raw = [sources_raw]
                s_ok: List[Dict[str, int]] = []
                if isinstance(sources_raw, list):
                    for it in sources_raw:
                        cid = it.get("chunk_id") if isinstance(it, dict) else it
                        try:
                            cid_i = int(cid)
                        except Exception:
                            continue
                        if cid_i in valid_ids:
                            s_ok.append({"chunk_id": cid_i})
                s_ok = s_ok[:2]
                if not s_ok:
                    s_ok = _best_sources(f"{t} {stem}", k=2)

                cleaned.append({"type": "open_ended", "stem": stem, "hints": hints[:3], "sources": s_ok})
                if len(cleaned) >= qc:
                    break

            if cleaned:
                return TutorGenerateQuestionsData(
                    topic=t,
                    level=(level or "").strip() or None,
                    questions=cleaned,
                    sources=sources,
                    retrieval=rag.get("corrective") or {},
                ).model_dump()

    # Offline fallback: build questions from the extracted topic_profile.
    questions: List[Dict[str, Any]] = []

    defs = topic_profile.get("definitions") or []
    kps = topic_profile.get("key_points") or []
    exs = topic_profile.get("examples") or []
    misc = topic_profile.get("misconceptions") or []

    def _add(stem: str):
        stem = " ".join((stem or "").split()).strip()
        if len(stem) < 12:
            return
        if any(stem.lower() == q["stem"].lower() for q in questions):
            return
        questions.append({"type": "open_ended", "stem": stem, "hints": [], "sources": _best_sources(stem, k=2)})

    # Pick a few different angles based on what exists in the text.
    if isinstance(defs, list) and defs:
        d0 = defs[0]
        term = (d0.get("term") if isinstance(d0, dict) else "") or t
        _add(f"H√£y gi·∫£i th√≠ch '{term}' theo √Ω b·∫°n v√† n√™u m·ªôt v√≠ d·ª• minh ho·∫°.")

    if isinstance(kps, list) and kps:
        _add(f"Trong ch·ªß ƒë·ªÅ '{t}', h√£y t√≥m t·∫Øt 3 √Ω ch√≠nh quan tr·ªçng nh·∫•t v√† gi·∫£i th√≠ch v√¨ sao ch√∫ng quan tr·ªçng.")

    if isinstance(misc, list) and misc:
        m0 = misc[0]
        _add(f"N√™u m·ªôt hi·ªÉu l·∫ßm/sai l·∫ßm ph·ªï bi·∫øn li√™n quan ƒë·∫øn '{t}' v√† c√°ch tr√°nh.")

    if isinstance(exs, list) and exs:
        _add(f"H√£y ph√¢n t√≠ch v√≠ d·ª• trong t√†i li·ªáu li√™n quan ƒë·∫øn '{t}': m·ª•c ti√™u, c√°c b∆∞·ªõc/ch·ªçn l·ª±a ch√≠nh v√† k·∫øt qu·∫£.")

    # Fill remaining with general-but-not-fixed prompts.
    while len(questions) < qc:
        idx = len(questions) + 1
        _add(f"C√¢u {idx}: H√£y ƒë·∫∑t m·ªôt t√¨nh hu·ªëng th·ª±c t·∫ø v√† m√¥ t·∫£ c√°ch b·∫°n √°p d·ª•ng '{t}' ƒë·ªÉ gi·∫£i quy·∫øt.")
        if len(questions) >= qc:
            break

    return TutorGenerateQuestionsData(
        topic=t,
        level=(level or "").strip() or None,
        questions=questions[:qc],
        sources=sources,
        retrieval=rag.get("corrective") or {},
    ).model_dump()
